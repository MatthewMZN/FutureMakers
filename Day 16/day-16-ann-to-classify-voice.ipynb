{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Artificial Neural Network**\n","metadata":{}},{"cell_type":"markdown","source":"**AIM:**\nBefore we dive in let me give a brief of what we are upto. We have a dataset which based on certain paramaters classifies a voice based on gender. How do humans do it?\n> Sound waves travel into the ear canal until they reach the eardrum. The eardrum passes the vibrations through the middle ear bones or ossicles into the inner ear. The inner ear is shaped like a snail and is also called the cochlea. Inside the cochlea, there are thousands of tiny hair cells. Hair cells change the vibrations into electrical signals that are sent to the brain through the hearing nerve. The brain tells you that you are hearing a sound and what that sound is.\n> \n\nWhat happens in the brain is neurons perform certain operations to classify the sound, this is exactly what we will be trying to simulate. We will try to mimmic the functioning (on a much*1000 smaller scale) just to get the basic idea.\n","metadata":{}},{"cell_type":"markdown","source":"**Step 1**\n\nImport the basic libraries \nmatplotlib :: To plot graphs\n    numpy  :: To perform operations and manipulate arrays\n    pandas :: To read and manage the data from the file","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.081802Z","iopub.execute_input":"2021-08-01T12:52:57.082337Z","iopub.status.idle":"2021-08-01T12:52:57.087198Z","shell.execute_reply.started":"2021-08-01T12:52:57.082291Z","shell.execute_reply":"2021-08-01T12:52:57.086266Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Step 2**\n\nWe need to separate the dependent and independent variables. Here the first 20 set columns consists of the features and the last coloumn is the dependent variable, which takes to string values i.e Male and Female","metadata":{}},{"cell_type":"code","source":"dataset=pd.read_csv(\"../input/voice.csv\")\nX=dataset.iloc[:,0:20] #iloce is index location. Choosing all the rows and the first 20 columns\ny=dataset.iloc[:,-1].values #this is a numpyy array. \n\"\"\"-1 means the last column, -2 means 2nd to last, so on. This produces a string for male or female.\nNext step is to convert those strings into something the algorithm can work with\"\"\"\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.089721Z","iopub.execute_input":"2021-08-01T12:52:57.090233Z","iopub.status.idle":"2021-08-01T12:52:57.159947Z","shell.execute_reply.started":"2021-08-01T12:52:57.090176Z","shell.execute_reply":"2021-08-01T12:52:57.158764Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n\n          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n\n     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meanfreq</th>\n      <th>sd</th>\n      <th>median</th>\n      <th>Q25</th>\n      <th>Q75</th>\n      <th>IQR</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sp.ent</th>\n      <th>sfm</th>\n      <th>...</th>\n      <th>centroid</th>\n      <th>meanfun</th>\n      <th>minfun</th>\n      <th>maxfun</th>\n      <th>meandom</th>\n      <th>mindom</th>\n      <th>maxdom</th>\n      <th>dfrange</th>\n      <th>modindx</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.059781</td>\n      <td>0.064241</td>\n      <td>0.032027</td>\n      <td>0.015071</td>\n      <td>0.090193</td>\n      <td>0.075122</td>\n      <td>12.863462</td>\n      <td>274.402906</td>\n      <td>0.893369</td>\n      <td>0.491918</td>\n      <td>...</td>\n      <td>0.059781</td>\n      <td>0.084279</td>\n      <td>0.015702</td>\n      <td>0.275862</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066009</td>\n      <td>0.067310</td>\n      <td>0.040229</td>\n      <td>0.019414</td>\n      <td>0.092666</td>\n      <td>0.073252</td>\n      <td>22.423285</td>\n      <td>634.613855</td>\n      <td>0.892193</td>\n      <td>0.513724</td>\n      <td>...</td>\n      <td>0.066009</td>\n      <td>0.107937</td>\n      <td>0.015826</td>\n      <td>0.250000</td>\n      <td>0.009014</td>\n      <td>0.007812</td>\n      <td>0.054688</td>\n      <td>0.046875</td>\n      <td>0.052632</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.077316</td>\n      <td>0.083829</td>\n      <td>0.036718</td>\n      <td>0.008701</td>\n      <td>0.131908</td>\n      <td>0.123207</td>\n      <td>30.757155</td>\n      <td>1024.927705</td>\n      <td>0.846389</td>\n      <td>0.478905</td>\n      <td>...</td>\n      <td>0.077316</td>\n      <td>0.098706</td>\n      <td>0.015656</td>\n      <td>0.271186</td>\n      <td>0.007990</td>\n      <td>0.007812</td>\n      <td>0.015625</td>\n      <td>0.007812</td>\n      <td>0.046512</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.151228</td>\n      <td>0.072111</td>\n      <td>0.158011</td>\n      <td>0.096582</td>\n      <td>0.207955</td>\n      <td>0.111374</td>\n      <td>1.232831</td>\n      <td>4.177296</td>\n      <td>0.963322</td>\n      <td>0.727232</td>\n      <td>...</td>\n      <td>0.151228</td>\n      <td>0.088965</td>\n      <td>0.017798</td>\n      <td>0.250000</td>\n      <td>0.201497</td>\n      <td>0.007812</td>\n      <td>0.562500</td>\n      <td>0.554688</td>\n      <td>0.247119</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.135120</td>\n      <td>0.079146</td>\n      <td>0.124656</td>\n      <td>0.078720</td>\n      <td>0.206045</td>\n      <td>0.127325</td>\n      <td>1.101174</td>\n      <td>4.333713</td>\n      <td>0.971955</td>\n      <td>0.783568</td>\n      <td>...</td>\n      <td>0.135120</td>\n      <td>0.106398</td>\n      <td>0.016931</td>\n      <td>0.266667</td>\n      <td>0.712812</td>\n      <td>0.007812</td>\n      <td>5.484375</td>\n      <td>5.476562</td>\n      <td>0.208274</td>\n      <td>male</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset.iloc[:,-1])\nprint(dataset.iloc[:,-1].nunique())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.161315Z","iopub.execute_input":"2021-08-01T12:52:57.161581Z","iopub.status.idle":"2021-08-01T12:52:57.173592Z","shell.execute_reply.started":"2021-08-01T12:52:57.161535Z","shell.execute_reply":"2021-08-01T12:52:57.172242Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"0         male\n1         male\n2         male\n3         male\n4         male\n5         male\n6         male\n7         male\n8         male\n9         male\n10        male\n11        male\n12        male\n13        male\n14        male\n15        male\n16        male\n17        male\n18        male\n19        male\n20        male\n21        male\n22        male\n23        male\n24        male\n25        male\n26        male\n27        male\n28        male\n29        male\n         ...  \n3138    female\n3139    female\n3140    female\n3141    female\n3142    female\n3143    female\n3144    female\n3145    female\n3146    female\n3147    female\n3148    female\n3149    female\n3150    female\n3151    female\n3152    female\n3153    female\n3154    female\n3155    female\n3156    female\n3157    female\n3158    female\n3159    female\n3160    female\n3161    female\n3162    female\n3163    female\n3164    female\n3165    female\n3166    female\n3167    female\nName: label, Length: 3168, dtype: object\n2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Step 3**\n\nNow since we have the feature set and the set of dependent variables, We observe that the 'y' has strings and in maths we need values so what we will do is encode Male=1 and Female=0, (p.s I have nothing to do with assignments ;-) ). So we will use the LabelEncoder class and let it do its job.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder #Takes y outputs (labels) and converts them to 0s and 1s\nlabelencoder = LabelEncoder()\ny = labelencoder.fit_transform(y) #Gets the strings from the last column, male=1 and female=0\nprint(y)","metadata":{"_cell_guid":"0c27c093-b8d4-4bae-9896-4232d5b90e8a","_uuid":"714745ef3c404546a84aaa25e742fe53ac812dab","execution":{"iopub.status.busy":"2021-08-01T12:52:57.175457Z","iopub.execute_input":"2021-08-01T12:52:57.176060Z","iopub.status.idle":"2021-08-01T12:52:57.188358Z","shell.execute_reply.started":"2021-08-01T12:52:57.175986Z","shell.execute_reply":"2021-08-01T12:52:57.187156Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[1 1 1 ..., 0 0 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Step 4**\n\nNow if we observe the values in various columns we see that there is a problem either the values are extremely close to zero or the all the coloumn are of not the same scale. Why is there a need to Scale stuff might be the question, Well the answer is there is a lot of times where we will need to calculate slopes assume in the denominator two point are really close to zero, subtracting will lead it much more closer to zero and the slope assumes an amazingly huge value, so to prevent this kind of headache we generally use scaling in Neural Networks.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \n\"\"\"One column with very large values, versus another with small values. The computer tends to weigh\nthe former column greater due to higher values, which may not necessarily be the case.\nTherefore we standardized all the columns within the same scale. In order to do thiswe assume that\nwe have a normal distribution --> subtracts the mean from each resulting value then\ndivides them by the standard deviation\"\"\"\nX_sc = StandardScaler() #this is apart of sklearn\nX = X_sc.fit_transform(X) #different from abouve. SC is applied to the first 20 columns","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.190156Z","iopub.execute_input":"2021-08-01T12:52:57.190553Z","iopub.status.idle":"2021-08-01T12:52:57.207850Z","shell.execute_reply.started":"2021-08-01T12:52:57.190470Z","shell.execute_reply":"2021-08-01T12:52:57.206632Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Step 5**\n\nDivide the data into training set and test set, One set to train the neural Network and the other set to test the neural network.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.209579Z","iopub.execute_input":"2021-08-01T12:52:57.210003Z","iopub.status.idle":"2021-08-01T12:52:57.218769Z","shell.execute_reply.started":"2021-08-01T12:52:57.209926Z","shell.execute_reply":"2021-08-01T12:52:57.217692Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Step 6**\n\nNow starts the actual game of building the neural network for this we first import the required libraries.","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import History #Storing accuracy and loss information from each epoch\nfrom keras.utils import plot_model\nfrom keras.optimizers import SGD #Stochastic Gradient Descend. Review. \n\"\"\"This adds a bit of noise/randomness. It allows the algorithm to move in the direction that\nminimizes gradient of cost function (finding the global minimum).\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.220702Z","iopub.execute_input":"2021-08-01T12:52:57.221287Z","iopub.status.idle":"2021-08-01T12:52:57.236068Z","shell.execute_reply.started":"2021-08-01T12:52:57.221195Z","shell.execute_reply":"2021-08-01T12:52:57.235102Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'This adds a bit of noise/randomness. It allows the algorithm to move in the direction that\\nminimizes gradient of cost function (finding the global minimum).'"},"metadata":{}}]},{"cell_type":"markdown","source":"Before we get into the code let us try to understand the neural network structure we are aiming to build. What is the first thing we need to do if we want to understand or break down any sound? Simple we need to hear the sound in the coumputing terms we need to have an input. Here in the data set there are 20 paramaters which can also be called features and these are fed to the nodes on a one-to-one basis that is one node recieves one input.  We will call this the first layer and this is waht this piece of code does.\n> **classifier.add(Dense(output_dim=11,init='uniform',activation='relu',input_dim=20))**\n\nnext we pass this sound to the processing unit the brain where we have a lot of itermediate processing neurons before we actually get the output. In this case we will add just 2 intermediate stages of processing neurons with 11 nodes in each layer.\n> \n\n> **classifier.add(Dense(output_dim=11,init='uniform',activation='relu'))**\n\n\n> **classifier.add(Dense(output_dim=6,init='uniform',activation='relu'))**\n\nNow we need to get the output and one node will do the job\n\n> **classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))**\n\nIf you are wondering what is relu and sigmoid well these are the functions which are used to calculate the weights/loss etc. Do considering checking out what they stand for.\n\nIn the end we feed out data to the neural Network and wait for the magic to happen.","metadata":{}},{"cell_type":"code","source":"classifier=Sequential()\nhistory = History() #we are making an instance of the class History that was imported from keras.callbacks\n\"\"\"History itself is not a variable. This is immaterial with respect to the algorithm, \nbut when we create an instance, we materialize it and this is the only time this exists. This is like \npitching the idea of History to the computer, versus creating a prototype for the computer to work with\"\"\"\n#number of input variables =20\n#first layer \n#input_dim is only for the first layer\nclassifier.add(Dense(output_dim=11,init='uniform',activation='relu',input_dim=20)) \n#we choose output_dim via empirical data. We make sure the choice of dimensions does not give way to overfitting.\n\"\"\"'uniform' is used to initialize the values of the prescribed weights and keep them consistent \nthroughout one feedfoward process.\"\"\"\n#first Hidden layer\nclassifier.add(Dense(output_dim=11,init='uniform',activation='relu'))\n#Second Hidden\nclassifier.add(Dense(output_dim=6,init='uniform',activation='relu'))\n#output layer\nclassifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid')) #\n\"\"\"Sigmoid brings any number between 0 and 1,and is used in the last layer because probability is\n'read' between 0 and 1\"\"\"\n#Running the artificial neural network\nclassifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\"\"\"Adam optimizer reduces the step size in order to mitigate possibility of overstepping the minimum.\nSimilar to SGD but doesn't have randomness.\"\"\"\n#fitting\nclassifier.fit(X_train,y_train,batch_size=32,epochs=10,validation_split=0.1,callbacks=[history],shuffle=2) \n#validation within the training set. So 80% of the original dataset is training, then 10% of training is not for validation\n\"\"\"callbacks are functions used to monitor the way learning is done (whether it is improving or not\ne.g. model checkpoint. It saves the model at each epoch) History stores accuracy, validation loss, etc.\nEarly stopping stops the model from being trained to much with poor parameters that may cause \noverfitting. It will check to see the model's error or accuracy and stop if it performs \nunsatisfactorily\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:57.237539Z","iopub.execute_input":"2021-08-01T12:52:57.237816Z","iopub.status.idle":"2021-08-01T12:52:59.477102Z","shell.execute_reply.started":"2021-08-01T12:52:57.237778Z","shell.execute_reply":"2021-08-01T12:52:59.476170Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=20, units=11, kernel_initializer=\"uniform\")`\n  if __name__ == '__main__':\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\")`\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n  app.launch_new_instance()\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n","output_type":"stream"},{"name":"stdout","text":"Train on 2280 samples, validate on 254 samples\nEpoch 1/10\n2280/2280 [==============================] - 1s 238us/step - loss: 0.6898 - acc: 0.6741 - val_loss: 0.6755 - val_acc: 0.8740\nEpoch 2/10\n2280/2280 [==============================] - 0s 59us/step - loss: 0.5693 - acc: 0.8921 - val_loss: 0.4050 - val_acc: 0.8740\nEpoch 3/10\n2280/2280 [==============================] - 0s 66us/step - loss: 0.3033 - acc: 0.9026 - val_loss: 0.2441 - val_acc: 0.8898\nEpoch 4/10\n2280/2280 [==============================] - 0s 62us/step - loss: 0.1782 - acc: 0.9434 - val_loss: 0.1633 - val_acc: 0.9409\nEpoch 5/10\n2280/2280 [==============================] - 0s 57us/step - loss: 0.1258 - acc: 0.9645 - val_loss: 0.1421 - val_acc: 0.9488\nEpoch 6/10\n2280/2280 [==============================] - 0s 61us/step - loss: 0.0997 - acc: 0.9711 - val_loss: 0.1075 - val_acc: 0.9685\nEpoch 7/10\n2280/2280 [==============================] - 0s 64us/step - loss: 0.0874 - acc: 0.9746 - val_loss: 0.1176 - val_acc: 0.9685\nEpoch 8/10\n2280/2280 [==============================] - 0s 57us/step - loss: 0.0814 - acc: 0.9750 - val_loss: 0.1091 - val_acc: 0.9685\nEpoch 9/10\n2280/2280 [==============================] - 0s 62us/step - loss: 0.0798 - acc: 0.9768 - val_loss: 0.1111 - val_acc: 0.9646\nEpoch 10/10\n2280/2280 [==============================] - 0s 61us/step - loss: 0.0765 - acc: 0.9776 - val_loss: 0.1291 - val_acc: 0.9567\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"\"callbacks are functions used to monitor the way learning is done (whether it is improving or not\\ne.g. model checkpoint. It saves the model at each epoch) History stores accuracy, validation loss, etc.\\nEarly stopping stops the model from being trained to much with poor parameters that may cause \\noverfitting. It will check to see the model's error or accuracy and stop if it performs \\nunsatisfactorily\""},"metadata":{}}]},{"cell_type":"markdown","source":"**Final Step **\n\nIt is always important to see, what actually is happening and how the model is learning. So with every epoch there is some learning which happens. The model is capable of calculating the loss it is facing from the actual result and then correspondingly adjusts its weight in ","metadata":{}},{"cell_type":"code","source":"import sklearn.metrics as metrics \n#Now we will be predicting for test dataset and plotting loss/accuracy\n\"\"\"Using a package/library to evaluate the delta between true vs predicted labels (y output) to \nfind the error of the algorithm\"\"\"\ny_pred=classifier.predict(X_test) #Review\ny_pred = np.round(y_pred) #Integers","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:59.480557Z","iopub.execute_input":"2021-08-01T12:52:59.480843Z","iopub.status.idle":"2021-08-01T12:52:59.536095Z","shell.execute_reply.started":"2021-08-01T12:52:59.480794Z","shell.execute_reply":"2021-08-01T12:52:59.535068Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"y_pred #so it rounds  +- 0.5","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:59.537356Z","iopub.execute_input":"2021-08-01T12:52:59.537641Z","iopub.status.idle":"2021-08-01T12:52:59.564657Z","shell.execute_reply.started":"2021-08-01T12:52:59.537570Z","shell.execute_reply":"2021-08-01T12:52:59.563820Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"array([[ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.],\n       [ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.],\n       [ 0.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"history.history\n#This contains the information of acc/loss & val_acc/val_loss.","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:59.565816Z","iopub.execute_input":"2021-08-01T12:52:59.566191Z","iopub.status.idle":"2021-08-01T12:52:59.576309Z","shell.execute_reply.started":"2021-08-01T12:52:59.566145Z","shell.execute_reply":"2021-08-01T12:52:59.575373Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'acc': [0.6741228070175439,\n  0.89210526315789473,\n  0.90263157894736845,\n  0.94342105263157894,\n  0.96447368421052626,\n  0.97105263157894739,\n  0.97456140350877196,\n  0.97499999999999998,\n  0.97675438596491226,\n  0.97763157894736841],\n 'loss': [0.68983609634533261,\n  0.56929762572572939,\n  0.30334923016397575,\n  0.17823804330668952,\n  0.12578488687674205,\n  0.09966191440297846,\n  0.087431750747195461,\n  0.081360622495412829,\n  0.079810901639754309,\n  0.076478691444846619],\n 'val_acc': [0.87401574568485652,\n  0.87401574568485652,\n  0.88976377718091948,\n  0.94094488188976377,\n  0.94881889763779526,\n  0.96850393700787396,\n  0.96850393700787396,\n  0.96850393700787396,\n  0.96456692913385822,\n  0.95669291338582674],\n 'val_loss': [0.67550400626940987,\n  0.40497859655402779,\n  0.24405149599229256,\n  0.16331534433787262,\n  0.14209527407926836,\n  0.10753968286174019,\n  0.11757966613499668,\n  0.10907178807739668,\n  0.1110618155097633,\n  0.12912553477680355]}"},"metadata":{}}]},{"cell_type":"code","source":"print('Accuracy we are able to achieve with our ANN is',metrics.accuracy_score(y_pred,y_test)*100,'%')\n\nplt.plot(history.history['loss'], color = 'red',label='Variaton Loss over the epochs',)\n\"\"\"'history' is the space or variable used to store values. It is defined as an \ninstance of callbacks\"\"\"\n#history is an instance of the history class\nplt.plot(history.history['acc'],color='cyan',label='Variation in Profit over the epochs')\n\nplt.xlabel('Epochs')\nplt.title('Loss/Accuracy VS Epoch')\nplt.ylabel('Loss/Accuracy')\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:52:59.577556Z","iopub.execute_input":"2021-08-01T12:52:59.577844Z","iopub.status.idle":"2021-08-01T12:52:59.732963Z","shell.execute_reply.started":"2021-08-01T12:52:59.577798Z","shell.execute_reply":"2021-08-01T12:52:59.732073Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Accuracy we are able to achieve with our ANN is 97.6340694006 %\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<matplotlib.figure.Figure at 0x7f847c8ed5f8>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkoIxxpgASwrGGGMCLCkYY4wJ+H8Rz0G7E9THGAAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"markdown","source":"If we observe the graph, Over a period of time it clear that the loss is gradually hitting zero and the Accuracy is increasing at a considerable rate.","metadata":{}}]}